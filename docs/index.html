<html lang="en"></html>
    <head>
        <meta charset="utf-8">
        <title>A Historical Analysis of the Guatemala National Police Archives</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="A Historical Analysis of the Guatemala National Police Archives">
        <link rel="stylesheet" href="styles.css">
    </head>
    <body>
        <h1>A Historical Analysis of the Guatemala National Police Archives</h1>
        
        <h2>About the Archive</h2>

            <blockquote cite="https://ahpn.lib.utexas.edu/microfilm">
                In addition to its 80 kilometers of paper documents, the Archivo Histórico de la Policía Nacional holds a number of reels of microfilm. Specific information on the content
                of these reels is currently incomplete and it is unclear if the documents on film also exist on paper in the archive. These microfilm reels are not yet indexed, but sample
                digitized images of the film can be downloaded below. We plan to provide access to additional reels of the AHPN microfilm here in the future.</blockquote>
            <br><em>— Digital Archive of the Guatemalan National Police Historical Archive <a href=https://ahpn.lib.utexas.edu/microfilm>(source)</a></em>

        <h2>Procedure</h2>

        <p>To follow the steps below, the following must be installed:</p>

        <ul>
            <li><pre>wget</pre></li>
            <li>R Studio</li>
            <li><pre>bash</pre> or PowerShell</li>
            <li>A web browser</li>
        </ul>

        Once the above software was installed, the following steps were followed:

        <ol>
            <li>Create a text file "wget-urls.txt" with the list of <a href="https://ahpn.lib.utexas.edu/microfilm">seven microfilm PDFs</a> to download for analysis.</li>
            <li>In "wget-output", run <pre>wget -i ../wget-urls.txt -r --no-parent -nd -w 2 --limit-rate=10</pre>.</li>
            <li>Open R Studio and navigate to archive-analysis/ocr-output. Set that folder as your working directory.</li>
            <li>Run all lines in ocr-pdf.R. This will take awhile, as there are 1,686 pages to OCR among all the PDFs. The R script first converts each page to TIFF then output text files with the contents of each page of the PDFs.</li>
            <li>Open a command line in the ocr-output directory. Run <pre>rm *.tiff</pre> (<pre>bash</pre>) or <pre>del *.tiff</pre> (PowerShell) to remove all the lingering TIFF files.</li>
            <li>From the command line (e.g. <pre>bash</pre>), run <pre>cat * > ../merged-text.txt</pre> to merge all the OCR'd text files into one file. If using Powershell, try <pre>type * > ..\merged-text.txt</pre></li>
            <li>Visit <a href="https://www.databasic.io/en/wordcounter/">DataBasic's WordCounter page</a>, click "upload a file", select "merged-text.txt" and click "COUNT".</li>li>
        </ol>


        <h2>Conclusion</h2>

        <p>
           Unfortunately, using R to convert and OCR so many PDFs was unstable, so R was not able to get through all the files.
           Further, the quality of the files was unsuitable for OCR, resulting in most of the OCR'd text being gibberish and incomplete; not even any Spanish could be made out!
        </p>

        <p>If you're curious, you can still view a sample WordCounter output <a href=https://www.databasic.io/en/wordcounter/results/5ee982bef0fc7e01fdb60c5f>here</a>.</p>
    </body>
</html>
