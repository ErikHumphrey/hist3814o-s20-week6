<html lang="en"></html>
    <head>
        <meta charset="utf-8">
        <title>A Historical Analysis of the Guatemala National Police Archives</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="A Historical Analysis of the Guatemala National Police Archives">
        <link rel="stylesheet" href="styles.css">
    </head>
    <body>
        <h1>A Historical Analysis of the Guatemala National Police Archives</h1>
        <div class="subtitle">by Erik Humphrey</div>
        
        <p>This historical analysis aims to uncover the most common words used on the microfilm from the Guatemala National Police Archives.</p>

        <h2>About the microfilm reels</h2>

            <blockquote cite="https://ahpn.lib.utexas.edu/microfilm">
                In addition to its 80 kilometers of paper documents, the Archivo Histórico de la Policía Nacional holds a number of reels of microfilm. Specific information on the content
                of these reels is currently incomplete and it is unclear if the documents on film also exist on paper in the archive. These microfilm reels are not yet indexed, but sample
                digitized images of the film can be downloaded below. We plan to provide access to additional reels of the AHPN microfilm here in the future.</blockquote>
            <br><em>— Digital Archive of the Guatemalan National Police Historical Archive <a href=https://ahpn.lib.utexas.edu/microfilm>(source)</a></em>

        <h2>Procedure</h2>

        <p>After <a href="https://github.com/ErikHumphrey/hist3814o-s20-week6">downloading the repository</a>, the following must be installed:</p>

        <ul>
            <li><code>wget</code></li>
            <li>R Studio</li>
            <li><code>bash</code> or PowerShell</li>
            <li>A web browser</li>
        </ul>

        With the above software installed, these steps were followed to complete the analysis:

        <ol>
            <li>Create a text file "wget-urls.txt" with the list of <a href="https://ahpn.lib.utexas.edu/microfilm">seven microfilm PDFs</a> to download for analysis.</li>
            <li>In "wget-output", run <code>wget -i ../wget-urls.txt -r --no-parent -nd -w 2 --limit-rate=10</code>.</li>
            <li>Open R Studio and navigate to archive-analysis/ocr-output. Set that folder as your working directory.</li>
            <li>Run all lines in ocr-pdf.R. This will take awhile, as there are 1,686 pages to OCR among all the PDFs. The R script first converts each page to TIFF then output text files with the contents of each page of the PDFs.</li>
            <li>Open a command line in the ocr-output directory. Run <code>rm *.tiff</code> (<code>bash</code>) or <code>del *.tiff</code> (PowerShell) to remove all the lingering TIFF files.</li>
            <li>From the command line (e.g. <code>bash</code>), run <code>cat * > ../merged-text.txt</code> to merge all the OCR'd text files into one file. If using Powershell, try <code>type * > ..\merged-text.txt</code></li>
            <li>Visit <a href="https://www.databasic.io/en/wordcounter/">DataBasic's WordCounter page</a>, click "upload a file", select "merged-text.txt" and click "COUNT".</li>
        </ol>


        <h2>Conclusion</h2>

        <p>
           Unfortunately, using R to convert and OCR so many PDFs was unstable, so R was not able to get through all the files.
           Further, the quality of the files was unsuitable for OCR, resulting in most of the text generated from the OCR being incomprensible, sparse, and incomplete. Next to no Spanish words could be made out.
        </p>

        <p>If you're curious, you can still view a sample WordCounter output <a href=https://www.databasic.io/en/wordcounter/results/5ee982bef0fc7e01fdb60c5f>here</a>.</p>
    </body>
</html>
